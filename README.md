# ‚öì Motivation 
![image](https://github.com/user-attachments/assets/e7a14035-ea33-46d1-a3c0-271202e7915e)

# üç∞Contributions
(1) A new benchmark, MV-RGBT, is collected to make it representative of multi-modal warranting scenarios, filling the gap between the data in current benchmarks and imaging conditions which motivate RGBT tracking.

(2) A new problem, `when to fuse', is posed to develop reliable fusion strategies for RGBT trackers, as in MMW scenarios multi-modal information fusion may be counterproductive. To facilitate its discussion, a new solution, MoETrack, with multiple tracking experts is proposed. It performs state-of-the-art on several benchmarks, including MV-RGBT, LasHeR, and VTUAV-ST.

(3) A new compositional perspective for method evaluation is provided by categorising MV-RGBT into two subsets, MV-RGBT-RGB and MV-RGBT-TIR, promoting a novel in-depth analysis and offering insightful recommendations for future developments in RGBT tracking.

# ‚òï MoETrack
![1941d3893289ae7e164d7c0df6cece1](https://github.com/user-attachments/assets/bc661fca-9e6e-47b3-8349-b1835fa57b57)

The novelty of MoETrack is two-folds: (1) The joint training of multiple experts, leading to more reliable predictions generated from each expert, and (2) the adaptive modality switcher significantly improve the tracking robustness, especially encountering multi-modal warranting scenarios. 

# ü•áPerformance
![80d01bedc38d233982d1f14b42d07d4](https://github.com/user-attachments/assets/850a447c-ed4a-4950-82bc-f526be2d9818)

![71978ffc86eb9851e24a7e8353474ef](https://github.com/user-attachments/assets/6e6e46eb-2710-41cd-8611-c1946131f1ac)

![6c689cfcfcea186e7874978b93fb43c](https://github.com/user-attachments/assets/f693ebcf-575d-40d0-b080-e6b4019c337e)


‚≠ê More detailed introduction of the dataset will be available [here](https://github.com/Zhangyong-Tang/MVRGBT)

ü´µFind our survey work at another [repo](https://github.com/Zhangyong-Tang/Survey-for-MultiModal-Visual-Object-Tracking)

